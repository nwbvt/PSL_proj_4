{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal 1\n",
    "1. This system would first center all ratings from each user by subtracting the mean rating for the user from each rating. This would ensure that users who tend to rate everything consistently higher give the same results as users that are consistently more negative. \n",
    "2. Then it would select all movies in the given genre.\n",
    "3. Then it would filter out all movies that have fewer than a given threshold of ratings to eliminate movies that only had a few (perhaps overly positive) ratings.\n",
    "4. Then it would calculated the average centered rating for each selected movie and return the top _n_ movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal 2\n",
    "1. Again this system would center all ratings by subtracting the mean rating.\n",
    "2. Then it would calculate the average centered rating for each user for each genre.\n",
    "3. Then it would select the _m_ users with the top average ratings for the given genre. This should give the users that are the biggest fans of the genre.\n",
    "4. Then using only those _m_ users, find the average rating for each movie, and return the top _n_ movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User based collaborative recomendations\n",
    "\n",
    "For this algorithim we find the top _m_ users with similar ratings to the new ratings using the cosine similarity metric. We then use their ratings to find to top rated _n_ movies the user has not already rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from data import load_movies_rated, load_user_ratings, load_movies\n",
    "from data import load_ratings, load_users\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_collaborative(new_ratings, user_ratings=None, movies=None, num_users=100):\n",
    "    \"\"\"\n",
    "    Get recommendations based on the user's ratings\n",
    "    \"\"\"\n",
    "    user_ratings = user_ratings if user_ratings is not None else load_user_ratings()\n",
    "    movies = movies if movies is not None else load_movies()\n",
    "    rated_movies = [int(m) for m in new_ratings.keys() if int(m) in user_ratings.columns]\n",
    "    relevant_ratings = user_ratings[rated_movies]\n",
    "    mean_rating = sum(new_ratings.values())/len(new_ratings)\n",
    "    new_vector = [rating-mean_rating for m, rating in new_ratings.items() if int(m) in user_ratings.columns]\n",
    "    similarities = cosine_similarity(relevant_ratings, [new_vector])\n",
    "    similar_users = [i for sim, i in sorted(zip(similarities[:,0], user_ratings.index), reverse=True)[:num_users]]\n",
    "    movie_ratings = user_ratings.loc[similar_users].mean()\n",
    "    movie_ratings.name = \"rating\"\n",
    "    ranked_movies = movies.join(movie_ratings).sort_values('rating', ascending=False)\n",
    "    return ranked_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item based collaborative recommendations\n",
    "\n",
    "This approach will use the training rankings to produce a similarity matrix for each movie using the cosine similarity metric. It will then take the highest ranked movies the user provided and the movies with the highest average mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_collaborative(new_ratings, movie_ratings=None, movies=None):\n",
    "    \"\"\"\n",
    "    Get recommendations based on similar movies to the user's favorites\n",
    "    \"\"\"\n",
    "    movie_ratings = movie_ratings if movie_ratings is not None else load_movie_ratings()\n",
    "    movies = movies if movies is not None else load_movies()\n",
    "    highest_rating = max(new_ratings.values())\n",
    "    rated_movies = [int(movie) for movie in new_ratings if int(movie) in movie_ratings.index]\n",
    "    index_list = movie_ratings.index.tolist()\n",
    "    favorite_movies = [index_list.index(movie) for movie in rated_movies if new_ratings[str(movie)] == highest_rating]\n",
    "    sims = cosine_similarity(movie_ratings)[favorite_movies,].mean(axis=0)\n",
    "    ranked = movies.join(pd.DataFrame({\"rating\": sims}, index=index_list)).sort_values(\"rating\", ascending=False)\n",
    "    ranked.drop(rated_movies, axis=0, inplace=True)\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since predicted ratings aren't particularly relevant compared to the ability of the system to predict whether or not the user would like the movie, I will treat this as a classification problem where the truth is whether or not the user give the movie is given an above average (for that user) rating. For each split group I will use the training users as the basis of the ratings and the test users as users of the system. Each training users's ratings will be split in half, with the first half being used to generate the recommendations and the second half used to test those recommendations. I will then compute the ROC-AUC for the scores assigned to those predictions relative to whether or not the user gave the actual movie an above average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(all_ratings, users, index):\n",
    "    all_ratings = all_ratings.join(users['split'], on=\"user\")\n",
    "    train = all_ratings[all_ratings['split'] != index]\n",
    "    test = all_ratings[all_ratings['split'] == index]\n",
    "    return train, test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_user(user_ratings, train_ratings, algorithm):\n",
    "    \"\"\"\n",
    "    Test a user's recommendations\n",
    "    \"\"\"\n",
    "    n_ratings = len(user_ratings)\n",
    "    use_ratings = {str(int(r['movie'])): r['rating'] for i, r in user_ratings[:n_ratings//2].iterrows()}\n",
    "    test_ratings = {int(r['movie']): r['rating'] for i, r in user_ratings[n_ratings//2:].iterrows()}\n",
    "    recs = algorithm(use_ratings, train_ratings)\n",
    "    rec_ratings = recs.loc[test_ratings]['rating']\n",
    "    rec_ratings.fillna(recs['rating'].min(), inplace=True)\n",
    "    mean_rating = user_ratings['rating'].mean()\n",
    "    pos_rating = [r > mean_rating for r in test_ratings.values()]\n",
    "    if sum(pos_rating) in {0, len(pos_rating)}:\n",
    "        return None\n",
    "    return roc_auc_score(pos_rating, rec_ratings)\n",
    "\n",
    "def run_test(test_data, train_ratings, algorithm):\n",
    "    score_sum = 0\n",
    "    n = 0\n",
    "    by_user = test_data.groupby('user')\n",
    "    for u, ratings in by_user:\n",
    "        user_auc = test_user(ratings, train_ratings, algorithm)\n",
    "        if user_auc is not None:\n",
    "            score_sum += user_auc\n",
    "            n = n+1\n",
    "    return score_sum/n\n",
    "        \n",
    "def run_tests_for_split(split, seed=19820618):\n",
    "    print(f\"For split {split}\")\n",
    "    ratings = load_ratings()\n",
    "    users = load_users()\n",
    "    users = shuffle(users, random_state=seed)\n",
    "    users['split'] = np.repeat(range(100), math.ceil(len(users)/100))[:len(users)]\n",
    "    train, test = get_split(ratings, users, split)\n",
    "    user_ratings = load_user_ratings(train)\n",
    "    movie_ratings = load_movie_ratings(train)\n",
    "    start = time.time()\n",
    "    user_based_auc = run_test(test, user_ratings, user_based_collaborative)\n",
    "    timetook = time.time() - start\n",
    "    print(f\"\\tUser based predictions had an AUC of {user_based_auc:.4f} and took {timetook:2f} seconds\")\n",
    "    start = time.time()\n",
    "    item_based_auc = run_test(test, movie_ratings, item_based_collaborative)\n",
    "    timetook = time.time() - start\n",
    "    print(f\"\\tItem based predictions had an AUC of {item_based_auc:.4f} and took {timetook:2f} seconds\")\n",
    "    return user_based_auc, item_based_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For split 0\n",
      "\tUser based predictions had an AUC of 0.7399 and took 6.008794 seconds\n",
      "\tItem based predictions had an AUC of 0.7461 and took 77.623325 seconds\n",
      "For split 1\n",
      "\tUser based predictions had an AUC of 0.7174 and took 5.908779 seconds\n",
      "\tItem based predictions had an AUC of 0.7317 and took 81.749648 seconds\n",
      "For split 2\n",
      "\tUser based predictions had an AUC of 0.7214 and took 5.933233 seconds\n",
      "\tItem based predictions had an AUC of 0.7193 and took 80.095006 seconds\n",
      "For split 3\n",
      "\tUser based predictions had an AUC of 0.7157 and took 6.286104 seconds\n",
      "\tItem based predictions had an AUC of 0.7120 and took 79.540403 seconds\n",
      "For split 4\n",
      "\tUser based predictions had an AUC of 0.7018 and took 6.277930 seconds\n",
      "\tItem based predictions had an AUC of 0.7184 and took 83.305579 seconds\n",
      "For split 5\n",
      "\tUser based predictions had an AUC of 0.7233 and took 5.685016 seconds\n",
      "\tItem based predictions had an AUC of 0.7414 and took 83.677603 seconds\n",
      "For split 6\n",
      "\tUser based predictions had an AUC of 0.7111 and took 6.039685 seconds\n",
      "\tItem based predictions had an AUC of 0.7152 and took 82.730924 seconds\n",
      "For split 7\n",
      "\tUser based predictions had an AUC of 0.7148 and took 6.020073 seconds\n",
      "\tItem based predictions had an AUC of 0.7327 and took 84.071999 seconds\n",
      "For split 8\n",
      "\tUser based predictions had an AUC of 0.7234 and took 6.091526 seconds\n",
      "\tItem based predictions had an AUC of 0.7387 and took 85.350959 seconds\n",
      "For split 9\n",
      "\tUser based predictions had an AUC of 0.7119 and took 5.999814 seconds\n",
      "\tItem based predictions had an AUC of 0.7353 and took 86.833898 seconds\n"
     ]
    }
   ],
   "source": [
    "ub_auc, ib_auc = zip(*[run_tests_for_split(i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User based Average: 0.7180734878066313\n",
      "Item based Average: 0.7290629974841013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Based</th>\n",
       "      <th>Item Based</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.739889</td>\n",
       "      <td>0.746109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717423</td>\n",
       "      <td>0.731710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.721366</td>\n",
       "      <td>0.719327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.715675</td>\n",
       "      <td>0.711961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.701808</td>\n",
       "      <td>0.718358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.723324</td>\n",
       "      <td>0.741372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.711096</td>\n",
       "      <td>0.715205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.714821</td>\n",
       "      <td>0.732675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.723424</td>\n",
       "      <td>0.738651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.711910</td>\n",
       "      <td>0.735262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"User based Average: {sum(ub_auc)/len(ub_auc)}\")\n",
    "print(f\"Item based Average: {sum(ib_auc)/len(ib_auc)}\")\n",
    "results = pd.DataFrame({\"Split\": range(1, 11), \"User Based\": ub_auc, \"Item Based\": ib_auc})\n",
    "results.set_index(\"Split\", inplace=True)\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both approaches produce similar results, with the item based prediction slightly outperforming user based prediction. However the user based predictions takes over an order of magnitude less time, so that is the approach I will go with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
